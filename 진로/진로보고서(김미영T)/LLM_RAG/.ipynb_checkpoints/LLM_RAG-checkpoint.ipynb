{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "274fbe3f-5810-4134-b62f-58e45c84b3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grandalf\n",
      "  Downloading grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pyparsing (from grandalf)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, grandalf\n",
      "Successfully installed grandalf-0.8 pyparsing-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcbed65b-f7f8-44d5-a58c-876548064398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwonj\\AppData\\Local\\Temp\\ipykernel_26232\\1375618389.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model='my_llm:latest',temperature=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='안녕하세요! 저는 AI 어시스턴트로서 여러분을 도와드리고 궁금한 점을 해결해드리기 위해 여기에 있습니다. 제 이름은 OpenAI에서 개발한 GPT-3라는 언어 모델을 기반으로 한 AI 어시스턴트입니다.\\r\\n\\r\\n저는 다양한 주제에 대해 배우고, 정보를 제공하며, 질문에 답하는 것을 즐깁니다. 정확하고 유용한 답변을 드리기 위해 최선을 다할 것이며, 항상 존중하고 예의 바른 태도를 유지하겠습니다.\\r\\n\\r\\n질문이 있거나 도움이 필요하시면 언제든지 물어보세요! 저는 여러분을 도와드리기 위해 여기에 있습니다.' additional_kwargs={} response_metadata={'model': 'my_llm:latest', 'created_at': '2025-03-13T10:40:52.4320746Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 133342913300, 'load_duration': 15985807500, 'prompt_eval_count': 61, 'prompt_eval_duration': 30145000000, 'eval_count': 120, 'eval_duration': 87175000000} id='run-bf0e8b94-fcfb-497a-aeef-e0e04dfc2636-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model='my_llm:latest',temperature=0)\n",
    "print(llm.invoke('안녕? 자기소개 해봐'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089eed4d-6139-4175-bb9d-230f887cd429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwonj\\AppData\\Local\\Temp\\ipykernel_26232\\4222088447.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model='my_llm:latest')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model='my_llm:latest')\n",
    "text_to_vector = embeddings.embed_query('안녕? 자기소개해봐')\n",
    "print(len(text_to_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b286524-3995-421e-9f6b-2d03886e0e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query와 Text1의 유사도는 0.63420928964504입니다.\n",
      "Query와 Text2의 유사도는 0.9999999999999976입니다.\n"
     ]
    }
   ],
   "source": [
    "query = embeddings.embed_query('안녕? 자기소개 해봐')\n",
    "text1 = embeddings.embed_query('내 이름은 박상인이야')\n",
    "text2 = embeddings.embed_query('안녕? 자기소개 해봐')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def calculate_cosine_similarity(list1, list2):\n",
    "    vec1 = np.array(list1).reshape(1, -1)\n",
    "    vec2 = np.array(list2).reshape(1, -1)\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "    return similarity[0][0]\n",
    "\n",
    "similarity1 = calculate_cosine_similarity(query, text1)\n",
    "similarity2 = calculate_cosine_similarity(query, text2)\n",
    "print(f'Query와 Text1의 유사도는 {similarity1}입니다.') # 유사도 : 0.6231\n",
    "print(f'Query와 Text2의 유사도는 {similarity2}입니다.') # 유사도 : 0.5138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5e22ac8-82dc-49f6-8b92-2860586682c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문과 가장 유사한 내용은 [Document(id='f08fdff0-0b40-4e50-a3be-5df8a03f78e8', metadata={}, page_content='대한민국 대통령은 윤석열입니다.')]입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "docs = [\"대한민국 대통령은 윤석열입니다.\",\"대한민국 국무총리는 한덕수입니다\"]\n",
    "vectorstore = FAISS.from_texts(docs,embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 1})\n",
    "print(f'질문과 가장 유사한 내용은 {retriever.invoke('국무총리 누구야?')}입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94ff6382-9861-491f-b5da-e1f19154b657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 2) RAG 사용 O =>  대한민국의 대통령은 윤석열이며, 국무총리는 한덕수입니다.\n",
      "\n",
      "신뢰도: 100%\n",
      "           +---------------------------------+         \n",
      "           | Parallel<context,question>Input |         \n",
      "           +---------------------------------+         \n",
      "                    **               **                \n",
      "                 ***                   ***             \n",
      "               **                         **           \n",
      "+----------------------+              +-------------+  \n",
      "| VectorStoreRetriever |              | Passthrough |  \n",
      "+----------------------+              +-------------+  \n",
      "                    **               **                \n",
      "                      ***         ***                  \n",
      "                         **     **                     \n",
      "           +----------------------------------+        \n",
      "           | Parallel<context,question>Output |        \n",
      "           +----------------------------------+        \n",
      "                             *                         \n",
      "                             *                         \n",
      "                             *                         \n",
      "                  +--------------------+               \n",
      "                  | ChatPromptTemplate |               \n",
      "                  +--------------------+               \n",
      "                             *                         \n",
      "                             *                         \n",
      "                             *                         \n",
      "                      +------------+                   \n",
      "                      | ChatOllama |                   \n",
      "                      +------------+                   \n",
      "                             *                         \n",
      "                             *                         \n",
      "                             *                         \n",
      "                   +-----------------+                 \n",
      "                   | StrOutputParser |                 \n",
      "                   +-----------------+                 \n",
      "                             *                         \n",
      "                             *                         \n",
      "                             *                         \n",
      "                +-----------------------+              \n",
      "                | StrOutputParserOutput |              \n",
      "                +-----------------------+              \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "query = '한국 대통령이랑 총리 알려줘.'\n",
    "template2 = ''' context를 기반으로 질문에 한국어로 간결하게 대답해줘. Context : {context}, Question: {question} '''\n",
    "prompt2 = ChatPromptTemplate.from_template(template2)\n",
    "retriever = vectorstore.as_retriever()\n",
    "llm_rag = {'context': retriever, 'question': RunnablePassthrough()} | prompt2 | llm | StrOutputParser()\n",
    "print(f'Case 2) RAG 사용 O => {llm_rag.invoke(query)}')\n",
    "print(llm_rag.get_graph().print_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7d97e6e-40e9-4400-a989-75f315423075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the USER_AGENT environment variable\n",
    "os.environ['USER_AGENT'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e56ab951-5470-421f-8252-7242de5401d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import bs4\n",
    "import ssl\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b94b449d-9b84-4aa4-a49f-04237074a8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물론이죠! 배고플 때 맛있는 점심을 드실 수 있는 몇 가지 추천 장소를 알려드릴게요:\n",
      "\n",
      "1. **한식당:** 한국 음식은 맛있고 든든해서 배고픔을 달래기에 완벽해요. 비빔밥, 김치찌개, 불고기 같은 요리를 시도해보세요.\n",
      "2. **분식집:** 분식은 빠르고 간편하게 먹을 수 있는 다양한 한식 요리로 구성되어 있어요. 떡볶이, 순대, 만두 등이 인기 메뉴예요.\n",
      "3. **중식당:** 중국 음식은 맛있고 든든해서 배고픔을 달래기에 좋아요. 짜장면, 짬뽕, 탕수육 같은 요리를 시도해보세요.\n",
      "4. **일식당:** 일식은 신선하고 건강한 재료로 만들어져 만족스러운 점심을 드실 수 있어요. 스시, 라멘, 돈가츠 같은 요리가 인기가 많아요.\n",
      "5. **양식당:** 양식은 다양한 맛과 스타일로 구성되어 있어서 취향에 맞는 음식을 찾을 수 있어요. 파스타, 피자, 버거 등이 인기 메뉴예요.\n",
      "6. **분식집:** 분식은 빠르고 간편하게 먹을 수 있는 다양한 한식 요리로 구성되어 있어요. 떡볶이, 순대, 만두 등이 인기가 많아요.\n",
      "7. **카페:** 카페는 커피와 차뿐만 아니라 간단한 식사도 제공해요. 샌드위치, 파니니, 케이크 같은 요리가 인기 메뉴예요.\n",
      "8. **패스트푸드점:** 패스트푸드는 빠르고 간편하게 먹을 수 있는 다양한 음식으로 구성되어 있어요. 버거, 치킨, 감자튀김 등이 인기가 많아요.\n",
      "9. **분식집:** 분식은 빠르고 간편하게 먹을 수 있는 다양한 한식 요리로 구성되어 있어요. 떡볶이, 순대, 만두 등이 인기가 많아요.\n",
      "10. **뷔페:** 뷔페는 다양한 음식으로 구성되어 있어서 취향에 맞는 음식을 찾을 수 있어요. 스시, 중식, 양식 등 다양한 종류의 음식이 제공돼요.\n",
      "\n",
      "이 추천들이 도움이 되길 바랍니다! 다른 질문이 있거나 더 도와드릴 것이 있다면 알려주세요.\n"
     ]
    }
   ],
   "source": [
    "# chain_level_1 : 입력 → LLM → 출력\n",
    "# 입력 정의(ver1)\n",
    "input_prompt = PromptTemplate(template=\"너는 {question}에 대해 한국어로 친절하고 간결하게 대답하는 인공지능이야\", input_variables=[\"question\"])\n",
    "\n",
    "# 입력 정의(ver2)\n",
    "template = \"너는 {question}에 대해 한국어로 친절하고 간결하게 대답하는 인공지능이야\"\n",
    "input_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM 정의 : Ollama에서 설치했던 모델 입력\n",
    "llm = ChatOllama(model='my_llm:latest',temperature=0)\n",
    "\n",
    "# 출력 정의 : LLM의 출력을 문자열 형식으로 변환\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Chain 정의 : 버티컬 라인(|)을 통해서 Chain의 파이프 라인 정의\n",
    "chain_level_1 = {'question': RunnablePassthrough()} | input_prompt | llm | output_parser\n",
    "\n",
    "# 출력 생성\n",
    "print(chain_level_1.invoke({\"question':'배고픈데 점심 추천해줘\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86a7204e-44e9-4f34-ba5b-b8e8e93bca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물론이죠! 배고플 때 맛있는 점심을 드실 수 있는 몇 가지 추천 장소를 알려드릴게요:\n",
      "\n",
      "1. **한식당:** 한국 음식은 맛있고 든든해서 배고픔을 달래기에 완벽해요. 비빔밥, 김치찌개, 불고기 같은 요리를 시도해보세요.\n",
      "2. **분식집:** 분식은 빠르고 간편하게 먹을 수 있는 다양한 음식을 제공해요. 떡볶이, 순대, 만두 등이 인기 메뉴예요.\n",
      "3. **카페:** 간단한 점심을 원한다면 카페에서 샌드위치나 파니니를 드셔보세요. 커피나 차도 함께 즐기실 수 있어요.\n",
      "4. **피자집:** 피자는 언제나 좋은 선택이에요! 다양한 토핑을 고르거나 클래식한 마르게리타 피자를 맛보세요.\n",
      "5. **버거킹:** 버거킹은 맛있는 버거와 감자튀김, 밀크쉐이크를 제공해요. 햄버거나 치킨샌드위치를 드셔보세요.\n",
      "6. **맥도날드:** 맥도날드는 다양한 메뉴로 유명해요. 빅맥이나 해피밀을 드셔보세요.\n",
      "7. **스타벅스:** 스타벅스는 커피와 차뿐만 아니라 간단한 식사 옵션도 제공해요. 파니니나 머핀을 드셔보세요.\n",
      "8. **서브웨이:** 서브웨이는 다양한 종류의 샌드위치를 제공해요. 치킨이나 스테이크를 고르거나, 야채만 들어간 샐러드를 드셔보세요.\n",
      "9. **치킨집:** 치킨은 언제나 좋은 선택이에요! 프라이드 치킨이나 양념 치킨을 드셔보세요.\n",
      "10. **피자헛:** 피자헛은 다양한 종류의 피자를 제공해요. 클래식한 페퍼로니나 마르게리타 피자를 드셔보세요.\n",
      "\n",
      "이것들은 배고픔을 달래줄 수 있는 몇 가지 추천 장소예요. 맛있게 드세요!\n"
     ]
    }
   ],
   "source": [
    "o1 = input_prompt.invoke({\"question':'배고픈데 점심 추천해줘\"})\n",
    "o2 = llm.invoke(o1)\n",
    "o3 = output_parser.invoke(o2)\n",
    "print(o3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7486508-9b5d-4f5e-b71d-b5e08332c96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6308\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# “SSL: CERTIFICATE_VERIFY_FAILED” 에러 나오면 사용\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "url= \"https://www.smartfn.co.kr/article/view/sfn202412230026\"\n",
    "# url= [\"https://www.smartfn.co.kr/article/view/sfn202412230026\",\"https://www.kipost.net/news/articleView.html?idxno=321448\"]\n",
    "loader = WebBaseLoader(url)\n",
    "loader.requests_kwargs = {\"verify\": False}\n",
    "docs = loader.load()\n",
    "print(len(docs[0].page_content))\n",
    "\n",
    "# Splitteer 정의 \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len, is_separator_regex=False)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(len(splits))\n",
    "\n",
    "# 벡터 스토어 정의\n",
    "embeddings = OllamaEmbeddings(model='my_llm:latest',temperature=0)\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa1ab3f0-8a25-442a-ae43-390472653c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현대자동차는 2024년도에 내수 시장의 지속적인 침체와 지정학적 불확실성 등 여러 도전과제에 직면할 것으로 예상됩니다. 그러나 북미 시장에서의 판매 호조, 전기차(EV) 및 고부가가치 차량의 성장, 그리고 글로벌 신용등급 향상이 이러한 어려움을 상쇄하는 데 도움이 될 수 있습니다.\n",
      "\n",
      "2024년 현대자동차의 영업 실적에 대한 구체적인 예측은 아직 나오지 않았지만, 몇 가지 주요 요인들을 고려해볼 수 있습니다:\n",
      "\n",
      "1. 내수 시장 침체: 국내 경제 둔화와 소비 불확실성으로 인해 내수 시장이 계속 어려움을 겪을 것으로 예상됩니다.\n",
      "2. 지정학적 불확실성: 미국과 중국 간의 무역 분쟁과 같은 글로벌 정치 및 경제적 위험이 현대자동차의 수익성에 영향을 줄 수 있습니다.\n",
      "3. 전기차(EV) 시장 성장: EV에 대한 수요가 증가함에 따라 현대자동차가 이 부문에서 상당한 성장을 이룰 것으로 예상됩니다.\n",
      "4. 고부가가치 차량 판매 증가: SUV와 같은 고급 차량에 대한 수요가 늘어나면서 현대자동차의 수익성이 개선될 수 있습니다.\n",
      "5. 글로벌 신용등급 향상: 2021년에 현대자동차가 세계 3대 신용평가사로부터 A등급을 받은 것은 회사의 재무 건전성을 강화하고 자금 조달 비용을 줄일 수 있습니다.\n",
      "\n",
      "이러한 요소들을 고려할 때, 현대자동차는 2024년도에 전반적으로 안정적인 영업 실적을 유지할 것으로 예상됩니다. 그러나 내수 시장의 지속적인 침체와 지정학적 불확실성은 수익성에 영향을 줄 가능성이 있습니다. 전기차 시장 성장과 고부가가치 차량 판매 증가로 인해 회사는 이러한 도전을 상쇄할 수 있을 것입니다.\n",
      "\n",
      "결론적으로, 현대자동차의 2024년도 영업 실적은 내수 시장의 지속적인 침체와 지정학적 불확실성 등 여러 요인에 의해 영향을 받을 것으로 예상됩니다. 그러나 전기차 시장 성장과 고부가가치 차량 판매 증가로 인해 회사는 이러한 도전을 상쇄할 수 있을 것입니다.\n",
      "\n",
      "신뢰도: 80%\n"
     ]
    }
   ],
   "source": [
    "# chain_level_2 : 질문 → 관련 정보 검색 → 입력 → LLM → 출력\n",
    "# 정보 검색기 정의\n",
    "retriever = vectorstore.as_retriever(search_type='mmr', search_kwargs={'k': 3})\n",
    "\n",
    "# 입력 정의 : 질문 + 관련 정보\n",
    "template = \"context를 기반으로 질문에 한국어로 간결하게 대답해줘. Context : {context}, Question: {question}\"\n",
    "input_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM 정의 : Ollama에서 설치했던 모델 입력\n",
    "llm = ChatOllama(model='my_llm:latest', temperature=0)\n",
    "\n",
    "# 출력 정의 : LLM의 출력을 문자열 형식으로 변환\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 검색 정보 후처리 : 검색된 관련 정보들을 하나의 문단으로 합침\n",
    "def merge_info(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain 정의 : 버티컬 라인(|)을 통해서 Chain의 파이프 라인 정의\n",
    "chain_level_2 = {'question': RunnablePassthrough(), 'context': retriever | merge_info} | input_prompt | llm | output_parser\n",
    "\n",
    "# 출력 생성\n",
    "print(chain_level_2.invoke({'question':'현대차 2024년도 영업 실적에 대해서 말해봐.'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459ff56-c91e-44b1-a6d2-0713039f6656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_RAG",
   "language": "python",
   "name": "llm_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
